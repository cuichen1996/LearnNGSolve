{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.5.2 Basics of MPI-parallel NGSolve\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_procs = '20'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from usrmeeting_jupyterstuff import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_cluster()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_cluster(num_procs)\n",
    "connect_cluster()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using MPI through NGSolve\n",
    "For convenience, NGSolve provides access to a little bit of MPI-functionality through the python interface.\n",
    "\n",
    "However, the general philosophy is that NGSolve should handle MPI-stuff on C++ side and not require the user\n",
    "to directly use it.\n",
    "\n",
    "In cases where more MPI functionality is needed, mpi4py can be used.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "MPI_Init returns a wrapper around the MPI-communcator used in NGSolve.\n",
    "\n",
    "It provides some basic functionality, for example it can tell us the number of\n",
    "procs in it, and give us the rank of each one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "from ngsolve import *\n",
    "comm = MPI_Init()\n",
    "print(\"Hello from rank \", comm.rank, ' of ', comm.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, \"comm\" provides:\n",
    "\n",
    "- time measurement\n",
    "- barriers\n",
    "- computing sums, minima, maxima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "t = comm.WTime()\n",
    "s2 = comm.Sum(1)\n",
    "t = comm.Max(comm.WTime()-t)\n",
    "if comm.rank==0:\n",
    "    print('There are ', s2, ' of us, which took us ', round(t,6), 'seconds to figure out')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed Meshes\n",
    "When we load a mesh from a file in parallel, it gets distributed among the ranks and each one gets only a part of it, \n",
    "**rank 0 gets nothing**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "mesh = Mesh('square.vol')\n",
    "print('rank', str(comm.rank)+\"'s part of the mesh has \", mesh.ne, 'elements, ', \\\n",
    "      mesh.nface, 'faces, ', mesh.nedge, 'edges and ', mesh.nv, ' vertices')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![square_apart](square_apart.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the entire geometry information is present everywhere:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px --targets 0:5\n",
    "print('rank', comm.rank, 'Materials:', mesh.GetMaterials())\n",
    "print('rank', comm.rank, 'Boundaries: ', mesh.GetBoundaries())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed Finite Element Spaces\n",
    "When we define a Finite Element Space on a distributed mesh, each rank constructs a\n",
    "Finite Element Space on it's part of the mesh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "fes = H1(mesh, order=3, dirichlet='bottom|left')\n",
    "print('fes on rank', comm.rank, 'has', fes.ndof, 'DOFs, globally we have ', fes.ndofglobal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "nd2 = comm.Sum(fes.ndof)\n",
    "if comm.rank==0:\n",
    "    print('Strangely, the sum of all local DOFs is ', nd2, '!=', fes.ndofglobal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we just sum up the dimensions of the local spaces $V^i$, we get the dimension of \n",
    "$\\Pi_i V^i$ and not the dimension of \n",
    "\n",
    "$$\n",
    "V = \\Pi_i V^i \\cap C^0(\\Omega)\n",
    "$$\n",
    "\n",
    "Some base functions have to be shared across subdomains. Each subdomain takes the place of a macro Finite Element.\n",
    "![bf_apart](bf_apart.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Information about how the DOFs stick together on a global level are stored in \n",
    "the \"ParallelDofs\" object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "pd = fes.ParallelDofs()\n",
    "print('rank', comm.rank, 'has', pd.ndoflocal, 'local DOFs, globally we have', pd.ndofglobal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can find out which DOFs are shared with which ranks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px --target=3\n",
    "print('I am rank ', comm.rank)\n",
    "print('---')\n",
    "\n",
    "for k in range(min(10,fes.ndof)):\n",
    "    print('I share DOF', k, 'with ranks:', [p for p in pd.Dof2Proc(k)])\n",
    "    \n",
    "print('... and so forth ...')\n",
    "print('\\n')\n",
    "\n",
    "for p in range(0, comm.size-1):\n",
    "    if len(pd.Proc2Dof(p)):\n",
    "        print('DOFs I share with rank', p, ': ', [p for p in pd.Proc2Dof(p)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a couple of points to consider here:\n",
    "\n",
    " - Locally, DOFs are numbered 0..ndoflocal-1.\n",
    " - There is no global enumeration!\n",
    " - The local numbering of DOFs is conistent across subdomain boundaries.\n",
    "   (This just means that there exists some global enumeration of DOFs that is cinsistent with the local ones.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed Weak Formulations & Linear Algebra\n",
    "\n",
    "Linear- or Bilinearforms can be split into subdomain contributions.\n",
    "\n",
    "For example, the usual bilinear form $a(\\cdot, \\cdot)$ associated to Poisson's equation can be split into\n",
    "$a_i(\\cdot, \\cdot)$ defined by:\n",
    "$$\n",
    "a(u,v) = \\sum_i a_i(u, v) = \\sum_i \\int_{\\Omega_i} \\nabla u \\nabla v~dx = \\sum_i a(u_{|\\Omega_i}, v_{|\\Omega_i})\n",
    "$$\n",
    "\n",
    "When we write down BLFs and LFs for distributed FESpace, we actually simply write down\n",
    "it's local contributions. \n",
    "\n",
    "The FESpace figures out how to stick them together to form global forms. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "u,v = fes.TnT()\n",
    "a = BilinearForm(fes)\n",
    "a += SymbolicBFI(grad(u)*grad(v))\n",
    "a.Assemble()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see what we get after assembling the bilinear form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px --target=1\n",
    "print('a.mat is a', type(a.mat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel Matrices and Vectors\n",
    "\n",
    "The general principle for distributed linear algebra objects is:\n",
    "\n",
    "*Parallel Object = Local Object + ParallelDofs*\n",
    "\n",
    "### Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px --target=1,2\n",
    "print('a.mat.local_mat on rank', comm.rank, 'is a', type(a.mat.local_mat), 'of dimensions', a.mat.local_mat.height, a.mat.local_mat.width)\n",
    "print('lcoal fes ndof: ', fes.ndof)\n",
    "print('a.mat.row_pardofs: ', a.mat.row_pardofs)\n",
    "print('a.mat.col_pardofs: ', a.mat.col_pardofs)\n",
    "print('fes pardofs:       ', fes.ParallelDofs())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each rank assembles it's local contribution to the global bilinear form into a sparse matrix, with dimensions matching that of the *local* FESpace!\n",
    "\n",
    "Let us assume we have some global numbering, and assume that $I_k$ is the set of indices corresponding to DOFs\n",
    "on rank $k$. \n",
    "\n",
    "The ebmedding matrices $E_k\\in\\mathbb{R}^{n_i\\times n}$ take local vectors of dimension $n_k$ and gives us global vectors of dimension $n$ .\n",
    "\n",
    "The global matrix $A$, operating on vectors of dimension $n$, can be assembled from the local matrices in the same way\n",
    "we usually assemble our FEM matrices from element matrices:\n",
    "\n",
    "$$\n",
    "A = \\sum_i E_i A^{(i)} E_i^T\n",
    "$$\n",
    "\n",
    "Importantly, the local matrices are **not** simply diagonal blocks of the global matrix,  $A^i$ only has partial values for DOFs that are shared with another rank, $A^{(i)} \\neq E_i^T A E_i$.\n",
    "\n",
    "![mat_distr](mat_distr.png)\n",
    "\n",
    "\n",
    "#### Note\n",
    "**We never globally assemble** $A$**!!**\n",
    "\n",
    "A common approach used by other software packages is to actually assemble $A$ and distribute it by rows.\n",
    "\n",
    "### Vectors\n",
    "\n",
    "Things look very similar for parallel vectors, they are again implemented as short, local vectors that\n",
    "make up the global one:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px \n",
    "f = LinearForm(fes)\n",
    "f += SymbolicLFI(x*y*v)\n",
    "f.Assemble()\n",
    "gfu = GridFunction(fes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px --target 1\n",
    "print('length of vector:    ', len(gfu.vec))\n",
    "print('length of local vec: ', len(gfu.vec.local_vec))\n",
    "print('dim local fes:       ', fes.ndof)\n",
    "print('dim global fes:      ', fes.ndofglobal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parallel Vectors additionally have a \"ParallelStatus\", which can be:\n",
    "\n",
    "- **Cumulated**, whenhe local vectors $v^i$ are just restrictions of the global vector $v$:\n",
    "\n",
    "$$\n",
    "v^{(i)} = E_i^T v\n",
    "$$\n",
    "\n",
    "- **Distributed**, when, similarly to parallel matrices, the global vector is the sum of local contributions\n",
    "\n",
    "$$\n",
    "v = \\sum_i E_iv^{(i)}\n",
    "$$\n",
    "\n",
    "The vector of the linear form $f$ is a collection of locally assembled vectors, so it is distributed.\n",
    "\n",
    "The vector of the GridFunction gfu has been initialized with zeros, so it has consistent values, it is cumulated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px --target 1\n",
    "print('status f vec:         ', f.vec.GetParallelStatus())\n",
    "print('status vec.local_vec: ', f.vec.local_vec.GetParallelStatus())\n",
    "print('')\n",
    "print('status gfu vec:       ', gfu.vec.GetParallelStatus())\n",
    "print('status vec.local_vec: ', gfu.vec.local_vec.GetParallelStatus())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Multiplication of a parallel matrix with a cumulated vector gives a distributed one:*\n",
    "\n",
    "$$\n",
    "w = A v = (\\sum_i E_i A^{(i)} E_i^T) v = \\sum_i E_i A^{(i)} E_i^Tv = \\sum_i E_i A^{(i)}v^{(i)} = \\sum_i E_i w^{(i)}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "v = gfu.vec.CreateVector()\n",
    "w = gfu.vec.CreateVector()\n",
    "v[:] = 1.0\n",
    "w.data = a.mat * v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px --target 1\n",
    "print('status v: ', v.GetParallelStatus())\n",
    "print('status w: ', w.GetParallelStatus())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solvers and Preconditioners with MPI\n",
    "\n",
    "Not all solvers and preconditioners included in NGSolve also work with MPI, but many do:\n",
    "### Direct Solvers\n",
    "- masterinverse: Collect the entire matrix on the \"master\" and invert sequentially there.\n",
    "- MUMPS inverse: Distributed parallel inverse, scalability is limited.\n",
    "\n",
    "### Preconditioners\n",
    "- BDDC\n",
    "- 'hypre': Boomer AMG\n",
    "- 'hypre_ams': Auxiliary Maxwell Space AMG \n",
    "- 'local'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "c = Preconditioner(a, 'hypre')\n",
    "#c = Preconditioner(a, 'bddc', inverse='mumps')\n",
    "a.Assemble()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "gfu.vec.data = solvers.CG(mat=a.mat, pre=c.mat, rhs=f.vec, tol=1e-6, maxsteps=30, printrates=comm.rank==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_cluster()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
