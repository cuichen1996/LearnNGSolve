{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cb1aec0",
   "metadata": {},
   "source": [
    "# Introduction to MPI with mpi4py\n",
    "\n",
    "MPI (abbreviation for message passing interface) is the standard library for communication within a cluster of processors. The library is available for many languages, such as C or Python. In this tutorial we use MPI within jupyter notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19e2e6f-4c17-4b49-8c25-caf242249177",
   "metadata": {},
   "source": [
    "## The installation happens in three steps:\n",
    "* install MPI including mpi4py\n",
    "* install PETSc including petsc4py (needed for advanced tutorials)\n",
    "* install MPI-parallel ngsolve (shipped as binaries starting from NGSolve 24.04)\n",
    "\n",
    "### Installing with conda\n",
    "\n",
    "The quickest path is using conda. Conda-forge provides binary packages for MPI and PETSc for many platforms and versions (TODO: list versions). Install anaconda (or mini-conda), and run something like\n",
    "\n",
    "    ~/miniconda3/bin/conda install mpi4py petsc4py\n",
    "    ~/miniconda3/bin/python3 -m pip install --pre --upgrade ngsolve\n",
    "\n",
    "\n",
    "### Installing conda-packages using pip\n",
    "\n",
    "* *Linux/MacOS* <br>\n",
    "\n",
    "      pip3 install -i https://pypi.anaconda.org/mpi4py/simple mpi4py==4.0.0.dev0 openmpi\n",
    "\n",
    "\n",
    "* *Windows* <br>\n",
    "\n",
    "      pip install impi_rt\n",
    "      pip install -i https://pypi.anaconda.org/mpi4py/simple mpi4py==4.0.0.dev0\n",
    "      \n",
    "\n",
    "### Installing MPI and PETSc without conda\n",
    "\n",
    "* *Linux:* <br>\n",
    "  install either one with your package manager: openmpi, mpich, or IntelMPI <br>\n",
    "  pip install mpi4py\n",
    "  \n",
    "* *MacOS:*\n",
    "\n",
    "      download [openmpi4.1.6](https://www.open-mpi.org//software/ompi/v4.1/)\n",
    "      ./configure\n",
    "      make all\n",
    "      sudo make all install\n",
    "      pip install mpi4py \n",
    "\n",
    "  \n",
    "* *Windows:* <br>\n",
    "  Install IntelMPI\n",
    "\n",
    "\n",
    "Install PETSc from a source-wheel:\n",
    "\n",
    "    export PETSC_CONFIGURE_OPTIONS=\"--with-fc=0 --with-debugging=0 --download-hypre\"\n",
    "    pip cache remove petsc \n",
    "    pip install --upgrade --no-deps --force-reinstall mpi4py petsc petsc4py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26bff21",
   "metadata": {},
   "source": [
    "## Using ipyparallel\n",
    "\n",
    "The `ipyparallel` module let us communicate with the cluster. The `Cluster` class represents the cluster. Every processor starts its own Python instance. You have to install the ipyparallel package:\n",
    "\n",
    "    !pip install ipyparallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80602cee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from ipyparallel import Cluster\n",
    "c = await Cluster(engines=\"mpi\").start_and_connect(n=4, activate=True)\n",
    "c.ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8de9f09-3ff1-4989-81a5-410002964aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mpi4py.MPI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9572e0e",
   "metadata": {},
   "source": [
    "We can define variables on the i$^{th}$ process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fe6731",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in c.ids:\n",
    "    c[i]['a'] = i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143b5cd9",
   "metadata": {},
   "source": [
    "Cells tagged with the %%px - magic are executed by the cluster processes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989f9149",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "b = a*a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69cd1a3c",
   "metadata": {},
   "source": [
    "query the results from the processes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f499c740",
   "metadata": {},
   "outputs": [],
   "source": [
    "c[:]['b']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9e2660",
   "metadata": {},
   "source": [
    "The MPI library\n",
    "---\n",
    "\n",
    "MPI provides functions to communicate within the cluster. The back-bone is a communicator object which knows about the participating processors. The standard communicator is the world-communicator where all processors of the cluster are included. \n",
    "\n",
    "The communicator knows the own number (called rank) within this group, and the size of the group:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b972c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "from mpi4py import MPI\n",
    "comm = MPI.COMM_WORLD\n",
    "print ('I am proc', comm.rank, 'within the team of', comm.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27932cb",
   "metadata": {},
   "source": [
    "We can send messages between processes using `send` and `recv`. With send we give the destination process where to send, and with recv we give the source processor number from where we expect data. We can send one Python object, which may also be a tuple or list.\n",
    "\n",
    "In this example, process $i$ is sending data to all processes with $j > i$. Then process $i$ is expecting data from processes with smaller ranks. This kind of communication is called point-to-point communication. Depending on the implementation of the mpi-library, and the object, the send and recv may or may not block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dacde84",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "fruits = ['apple', 'banana', 'clementine', 'durian', \\\n",
    "          'elderberries', 'figs', 'grapes', 'honeydew melon']\n",
    "for dst in range(comm.rank+1, comm.size):\n",
    "    comm.send(fruits[comm.rank%8], dest=dst)\n",
    "for src in range(comm.rank):\n",
    "    print (\"got a\", comm.recv(source=src))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a90e7a",
   "metadata": {},
   "source": [
    "If we want to send data to all processes in the cluster we use collective communication. A broadcast operation sends the same data from the root to everyone, a scatter operation splits a list to all processes. The default root is process 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b834df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "if comm.rank == 0:\n",
    "    comm.bcast(\"Hello from the boss!!\")\n",
    "    comm.scatter( [\"personal note to \"+str(i) for i in range(comm.size)])\n",
    "else:\n",
    "    print (comm.bcast(None))\n",
    "    print (comm.scatter(None))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b70893",
   "metadata": {},
   "source": [
    "The technology behind sending of Python objects is pickling. Every class which knows how to convert itself to and from a byte-stream (called serialization) can be exchanged via mpi communication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512a16cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "c.shutdown(hub=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
